{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ac63c395",
      "metadata": {
        "id": "ac63c395"
      },
      "source": [
        "# image Segmentation Model\n",
        "## Testing out the MobileNet model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4fbddd2",
      "metadata": {
        "id": "d4fbddd2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Conv2D, Activation, BatchNormalization, Flatten\n",
        "from keras.layers import UpSampling2D, Input, Concatenate, MaxPooling2D, SeparableConv2D, Resizing, Dropout, Conv2DTranspose,DepthwiseConv2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import layers, models\n",
        "from keras.models import Model\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.metrics import Recall, Precision\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gQNdnAEWYzjK",
      "metadata": {
        "id": "gQNdnAEWYzjK"
      },
      "source": [
        "Check for GPU and set up GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6edd930b",
      "metadata": {
        "id": "6edd930b"
      },
      "outputs": [],
      "source": [
        "tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available\", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db4a674c",
      "metadata": {
        "id": "db4a674c"
      },
      "outputs": [],
      "source": [
        "\"\"\"gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Restrict TensorFlow to only allocate a subset of the available memory on each GPU\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lLv1VcfVY9PL",
      "metadata": {
        "id": "lLv1VcfVY9PL"
      },
      "source": [
        "Date Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "091c6fa4",
      "metadata": {
        "id": "091c6fa4"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46dfcaca",
      "metadata": {
        "id": "46dfcaca"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "EPOCHS = 80\n",
        "BATCH = 8\n",
        "LR = 1e-4\n",
        "PATH = \"final_data\"\n",
        "# PATH = \"all_data-20240807T114638Z-001/all_data\"\n",
        "# PATH = \"25 Landmarks-20240609T154533Z-001/25 Landmarks\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b008d58b",
      "metadata": {
        "id": "b008d58b"
      },
      "outputs": [],
      "source": [
        "class CustomActivation(tf.keras.layers.Layer):\n",
        "    def __init__(self, a=1.0, b=2.0,  fun_name = None, **kwargs):\n",
        "        super(CustomActivation, self).__init__(**kwargs)\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.fun_name = fun_name\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.keras.layers.Activation('relu')(inputs)\n",
        "        if self.fun_name ==\"activation_a\":\n",
        "            outputs = 1 - tf.math.exp(-self.a * x)\n",
        "        if self.fun_name ==\"activation_b\":\n",
        "            outputs = tf.math.igamma(self.b, self.a*x)\n",
        "        if self.fun_name ==\"activation_c\":\n",
        "            outputs = 1/(1+(inputs/self.a)**(-self.b))\n",
        "        if self.fun_name == \"activation_d\":\n",
        "            outputs = (2 / tf.constant(np.pi, dtype=tf.float32)) * tf.atan(tf.exp(inputs * tf.constant(np.pi / 2, dtype=tf.float32)))\n",
        "        if self.fun_name == None:\n",
        "            outputs = x\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class CustomBatchScaling(tf.keras.layers.Layer):\n",
        "    def __init__(self,fun_name = None, trainable=True):\n",
        "        super(CustomBatchScaling, self).__init__()\n",
        "        self.trainable = trainable\n",
        "        self.fun_name = fun_name\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if self.fun_name ==\"activation_a\":\n",
        "            self.a = self.add_weight(name='a', shape=(input_shape[-1],),\n",
        "                                     initializer='ones', trainable=self.trainable)\n",
        "        else:\n",
        "            self.a = self.add_weight(name='a', shape=(input_shape[-1],),\n",
        "                                     initializer='ones', trainable=self.trainable)\n",
        "            self.b = self.add_weight(name='b', shape=(input_shape[-1],),\n",
        "                                    initializer='ones', trainable=self.trainable)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.keras.layers.Activation('relu')(inputs)\n",
        "        if self.fun_name ==\"activation_a\":\n",
        "            outputs = 1 - tf.math.exp(-self.a * x)\n",
        "        if self.fun_name ==\"activation_b\":\n",
        "            outputs = tf.math.igamma(self.b, self.a*x)\n",
        "        if self.fun_name ==\"activation_c\":\n",
        "            outputs = 1/(1+(inputs/self.a)**(-self.b))\n",
        "        if self.fun_name == None:\n",
        "            outputs = x\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(CustomBatchScaling, self).get_config()\n",
        "        config.update({\n",
        "            'trainable': self.trainable\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ab200b",
      "metadata": {
        "id": "52ab200b"
      },
      "source": [
        "## load the data (images as x_train and masks as y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0669aa30-f078-435a-817a-28b94d956233",
      "metadata": {
        "id": "0669aa30-f078-435a-817a-28b94d956233"
      },
      "outputs": [],
      "source": [
        "def load_data_with_matching_pairs(path, num_masks=3):\n",
        "    input_dir = os.path.join(path, \"data_images\")\n",
        "    target_dir = os.path.join(path, \"2D Team 1/data_heatmaps_shifted30\")\n",
        "\n",
        "    input_files = [fname for fname in os.listdir(input_dir)]\n",
        "    target_files = [fname for fname in os.listdir(target_dir)]\n",
        "\n",
        "    # Create a dictionary to hold the mask paths\n",
        "    masks = {}\n",
        "    for fname in input_files:\n",
        "        basename = os.path.splitext(fname)[0]\n",
        "        mask_files = []\n",
        "        for i in range(num_masks):\n",
        "            # Check each index for multiple file formats\n",
        "            found = False\n",
        "            for ext in ['.png', '.jpg', '.jpeg', '.JPG']:\n",
        "                potential_path = os.path.join(target_dir, f\"{basename}_{i}{ext}\")\n",
        "                if os.path.exists(potential_path):\n",
        "                    mask_files.append(potential_path)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "        masks[basename] = mask_files\n",
        "    input_basenames = set(os.path.splitext(fname)[0] for fname in input_files)\n",
        "    target_basenames = set(\"_\".join(fname.split('_')[:-1]) for fname in target_files)\n",
        "    common_files = input_basenames & target_basenames\n",
        "    images = []\n",
        "    mask_sets = []\n",
        "    for fname in common_files:\n",
        "        # if \"arpit\" in fname.lower():\n",
        "        #     continue\n",
        "        found = False\n",
        "        for ext in ['.png', '.jpg', '.jpeg', '.JPG']:\n",
        "            image_path = os.path.join(input_dir, f\"{fname}{ext}\")\n",
        "            if os.path.exists(image_path):\n",
        "                images.append(image_path)\n",
        "                mask_sets.append(masks[fname])\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            print(f\"Warning: No image found for {fname} in .png, .jpg, .jpeg, or .JPG format\")\n",
        "\n",
        "    print(f\"Found {len(images)} images and {len(mask_sets)} mask sets\")\n",
        "\n",
        "    if len(images) != len(mask_sets):\n",
        "        raise ValueError(\"Mismatch between the number of images and mask sets\")\n",
        "    return images, mask_sets\n",
        "\n",
        "image_path, mask_path = load_data_with_matching_pairs(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5qOkIypiXBhv",
      "metadata": {
        "id": "5qOkIypiXBhv"
      },
      "outputs": [],
      "source": [
        "def create_augmented_generator(X_train, y_train, batch, seed):\n",
        "\n",
        "    # Define data augmentation for images\n",
        "    image_datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0,\n",
        "        channel_shift_range = 0,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False,\n",
        "        fill_mode='constant',\n",
        "        cval=0\n",
        "    )\n",
        "\n",
        "    # Define data augmentation for masks\n",
        "    mask_datagen = ImageDataGenerator(\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0,\n",
        "        channel_shift_range = 0,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False,\n",
        "        fill_mode='constant',\n",
        "        cval=0\n",
        "    )\n",
        "\n",
        "    # Fit the generators to your data (if necessary)\n",
        "    image_datagen.fit(X_train, augment=True, seed=seed)\n",
        "    mask_datagen.fit(y_train, augment=True, seed=seed)\n",
        "\n",
        "    # Create the generators for images and masks\n",
        "    image_generator = image_datagen.flow(X_train, batch_size=batch, seed=seed)\n",
        "    mask_generator = mask_datagen.flow(y_train, batch_size=batch, seed=seed)\n",
        "    train_generator = zip(image_generator, mask_generator)\n",
        "\n",
        "    while True:\n",
        "        for (img, mask) in train_generator:\n",
        "            yield (img, mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c60a3c4a",
      "metadata": {
        "id": "c60a3c4a"
      },
      "outputs": [],
      "source": [
        "def read_image(path_or_array):\n",
        "    if isinstance(path_or_array, (str, bytes)):\n",
        "        # If it's a path, read the image\n",
        "        if isinstance(path_or_array, bytes):\n",
        "            path_or_array = path_or_array.decode('utf-8')  # Convert bytes to string\n",
        "        x = cv2.imread(path_or_array, cv2.IMREAD_COLOR)  # Read as color\n",
        "        x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
        "        x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))  # Resize the image\n",
        "        x = x / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    else:\n",
        "        # If it's already an array, use it directly\n",
        "        x = path_or_array\n",
        "\n",
        "    return x\n",
        "\n",
        "def read_mask(path_or_array):\n",
        "    if isinstance(path_or_array, (str, bytes)):\n",
        "        # If it's a path, read the mask\n",
        "        if isinstance(path_or_array, bytes):\n",
        "            path_or_array = path_or_array.decode('utf-8')  # Convert bytes to string\n",
        "        x = cv2.imread(path_or_array, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
        "        x = cv2.resize(x, (IMAGE_SIZE, IMAGE_SIZE))  # Resize the mask\n",
        "        x = x / 255.0  # Normalize to [0, 1]\n",
        "        x = np.expand_dims(x, axis=-1)  # Add a single channel\n",
        "\n",
        "    else:\n",
        "        # If it's already an array, use it directly\n",
        "        x = path_or_array\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237bfafa",
      "metadata": {
        "id": "237bfafa"
      },
      "outputs": [],
      "source": [
        "# Convert paths to images and masks\n",
        "import concurrent.futures\n",
        "\n",
        "def paths_to_images_and_masks(image_paths, mask_paths):\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "\n",
        "        images = list(executor.map(read_image, image_paths))\n",
        "\n",
        "        masks = []\n",
        "        for mask_set in mask_paths:\n",
        "            mask_set_images = list(executor.map(read_mask, mask_set))\n",
        "            combined_masks = np.stack(mask_set_images, axis=-1)\n",
        "            masks.append(combined_masks.squeeze(-2))\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "image_data, mask_data = paths_to_images_and_masks(image_path, mask_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be870fc8",
      "metadata": {
        "id": "be870fc8"
      },
      "outputs": [],
      "source": [
        "def augment_data(X_train, y_train, num_augmented=5):\n",
        "    # Define data augmentation parameters\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0,\n",
        "        shear_range=0.1,\n",
        "        zoom_range=0,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=False,\n",
        "        fill_mode='constant',\n",
        "        cval=0\n",
        "    )\n",
        "\n",
        "    # Preallocate numpy arrays for the augmented images\n",
        "    x_train_extra = np.empty((num_augmented * len(X_train), *X_train.shape[1:]))\n",
        "    y_train_extra = np.empty((num_augmented * len(y_train), *y_train.shape[1:]))\n",
        "\n",
        "    # Generate augmented images\n",
        "    for i in range(len(X_train)):\n",
        "        x_image = X_train[i]\n",
        "        x_image = np.expand_dims(x_image, axis=0)\n",
        "        y_image = y_train[i]\n",
        "        y_image = np.expand_dims(y_image, axis=0)\n",
        "\n",
        "        # Generate specified number of augmented versions of the image\n",
        "        image_it = datagen.flow(x_image, batch_size=1, seed=42)\n",
        "        mask_it = datagen.flow(y_image, batch_size=1, seed=42)\n",
        "\n",
        "        for j in range(num_augmented):\n",
        "            x_aug = next(image_it)\n",
        "            y_aug = next(mask_it)\n",
        "            augmented_x_image = x_aug[0]\n",
        "            augmented_y_image = y_aug[0]\n",
        "            x_train_extra[i*num_augmented + j] = augmented_x_image\n",
        "            y_train_extra[i*num_augmented + j] = augmented_y_image\n",
        "\n",
        "    # Using np.vstack for efficient concatenation\n",
        "    x_train_augmented = np.vstack([X_train, x_train_extra])\n",
        "    y_train_augmented = np.vstack([y_train, y_train_extra])\n",
        "\n",
        "    print(f\"Original number of images: {len(X_train)}, Augmented combined number of images: {len(x_train_augmented)}\")\n",
        "    print(f\"Original number of masks: {len(y_train)}, Augmented combined number of masks: {len(y_train_augmented)}\")\n",
        "\n",
        "    return x_train_augmented, y_train_augmented\n",
        "\n",
        "# Example usage\n",
        "image_data, mask_data = augment_data(image_data, mask_data, num_augmented=0)\n",
        "print(f\"Final number of images: {len(image_data)}, Final number of masks: {len(image_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q_CtvybCnHkp",
      "metadata": {
        "id": "Q_CtvybCnHkp"
      },
      "outputs": [],
      "source": [
        "# Slit data for train, validation and test\n",
        "train_x, test_x, train_y, test_y = train_test_split(image_data, mask_data, test_size=0.05, random_state=42)\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=42)\n",
        "print(f\"Train: {len(train_x)}, Validation: {len(valid_x)}, Test: {len(test_x)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6df618",
      "metadata": {
        "id": "7f6df618"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import RandomRotation\n",
        "\n",
        "class CustomRotationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_angle, **kwargs):\n",
        "        super(CustomRotationLayer, self).__init__(**kwargs)\n",
        "        self.max_angle = max_angle / 180  # Convert degrees to radians\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.input_spec = tf.keras.layers.InputSpec(shape=input_shape)\n",
        "        # Create the RandomRotation layer here\n",
        "        self.rotation_layer = RandomRotation(self.max_angle, fill_mode='reflect')\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            # Use the RandomRotation layer to rotate the images\n",
        "            rotated_images = self.rotation_layer(inputs)\n",
        "            return rotated_images\n",
        "        else:\n",
        "            return inputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(CustomRotationLayer, self).get_config()\n",
        "        config.update({'max_angle': self.max_angle * 180})  # Convert radians back to degrees for serialization\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d483f95e",
      "metadata": {
        "id": "d483f95e"
      },
      "outputs": [],
      "source": [
        "def dice_loss(y_true, y_pred):\n",
        "    smooth = 1e-15\n",
        "    num_channels = 3\n",
        "\n",
        "    def dice_coef_per_channel(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)  # Ensure y_true is float32\n",
        "        y_pred = tf.cast(y_pred, tf.float32)  # Ensure y_pred is float32\n",
        "        # intersection = tf.reduce_sum(y_true * y_pred)\n",
        "        intersection = tf.reduce_sum( tf.math.sqrt(y_true ) *  tf.math.sqrt (y_pred))\n",
        "        dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "        return dice\n",
        "\n",
        "    per_channel_losses = []\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        y_true_channel = tf.reshape(y_true[..., i], [-1])\n",
        "        y_pred_channel = tf.reshape(y_pred[..., i], [-1])\n",
        "        dice_loss_per_channel = 1 - dice_coef_per_channel(y_true_channel, y_pred_channel)\n",
        "        per_channel_losses.append(dice_loss_per_channel)\n",
        "\n",
        "    average_loss = tf.reduce_mean(per_channel_losses)\n",
        "\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(train_x, train_y, valid_x, valid_y, batch):\n",
        "    train_x_ = train_x.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    train_y_ = train_y.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "    valid_x_ = valid_x.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    valid_y_ = valid_y.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_x_, train_y_))\n",
        "    train_dataset = train_dataset.shuffle(len(train_x_)).batch(batch).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((valid_x_, valid_y_))\n",
        "    valid_dataset = valid_dataset.batch(batch).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    print(f\"Dataset Created Successfully\\nData Shape: Train: {train_x_.shape}, {train_y_.shape}\\nValidation: {valid_x_.shape}, {valid_y_.shape}\")\n",
        "\n",
        "    return train_dataset, valid_dataset\n",
        "train_dataset, valid_dataset = create_dataset(train_x, train_y, valid_x, valid_y, batch=BATCH)"
      ],
      "metadata": {
        "id": "1HrV5DVzDpDo"
      },
      "id": "1HrV5DVzDpDo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "snvIsj945pTI",
      "metadata": {
        "id": "snvIsj945pTI"
      },
      "outputs": [],
      "source": [
        "custom_layers = {\n",
        "    #\"CustomRotationLayer\": CustomRotationLayer,\n",
        "    \"dice_loss\": dice_loss\n",
        "}\n",
        "\n",
        "'''prev_model = tf.keras.models.load_model('SavedModels5/model_Feb_12_xyz_3cascade', custom_objects= custom_layers)\n",
        "prev_model.summary()\n",
        "\n",
        "def create_dataset(train_x, train_y, valid_x, valid_y, test_x, test_y, batch=BATCH):\n",
        "    # Ensure the shape of inputs\n",
        "    train_x = train_x.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    train_y= train_y.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    valid_x = valid_x.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    valid_y = valid_y.reshape(-1, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "    train_x_mask = (prev_model.predict(train_x) > 0.5)\n",
        "    valid_x_mask = (prev_model.predict(valid_x) > 0.5)\n",
        "    test_x_mask = (prev_model.predict(test_x) > 0.5)\n",
        "\n",
        "    final_train_x = train_x + train_x_mask\n",
        "    final_valid_x = valid_x + valid_x_mask\n",
        "    final_test_x = test_x + test_x_mask\n",
        "\n",
        "    train_dataset = create_augmented_generator(final_train_x, train_y, batch, seed=42)\n",
        "\n",
        "    valid_dataset = tf.data.Dataset.from_tensor_slices((final_valid_x, valid_y))\n",
        "    valid_dataset = valid_dataset.batch(batch)\n",
        "    valid_dataset = valid_dataset.repeat()\n",
        "    return valid_dataset, train_dataset, final_test_x\n",
        "\n",
        "valid_dataset, train_dataset, new_test_x = create_dataset(train_x, train_y, valid_x, valid_y, test_x, test_y, batch=BATCH)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WtQlQhr_AMcg",
      "metadata": {
        "id": "WtQlQhr_AMcg"
      },
      "outputs": [],
      "source": [
        "def display_batch(batch_images, batch_masks):\n",
        "    plt.figure(figsize=(10, 30))\n",
        "    for i in range(batch_images.shape[0]):\n",
        "        plt.subplot(batch_images.shape[0], 2, 2*i + 1)\n",
        "        plt.imshow(batch_images[i])\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(batch_images.shape[0], 2, 2*i + 2)\n",
        "        plt.imshow(batch_masks[i].squeeze(), cmap='gray')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get the first batch of images and masks\n",
        "train_iterator = iter(train_dataset)\n",
        "batch_images, batch_masks = next(train_iterator)\n",
        "#batch_images, batch_masks = next(train_dataset)\n",
        "\n",
        "# Display the batch\n",
        "display_batch(batch_images.numpy(), batch_masks.numpy())\n",
        "#display_batch(batch_images, batch_masks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y1PcmhaOZOax",
      "metadata": {
        "id": "y1PcmhaOZOax"
      },
      "source": [
        "Model Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7478eec3",
      "metadata": {
        "id": "7478eec3"
      },
      "outputs": [],
      "source": [
        "def create_model(image_size):\n",
        "    inputs = Input(shape=(image_size, image_size, 3), name=\"input_image\")\n",
        "\n",
        "    # Load the MobileNetV2 model with pre-trained weights\n",
        "\n",
        "    encoder = MobileNetV2(input_tensor=inputs, weights=\"imagenet\", include_top=False, alpha=0.5)\n",
        "\n",
        "    # Freeze/Not Freeze the weights of the MobileNetV2 model\n",
        "    for layer in encoder.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    skip_connection_names = [\"input_image\", \"block_1_expand_relu\", \"block_3_expand_relu\", \"block_6_expand_relu\"]\n",
        "    encoder_output = encoder.get_layer(\"block_13_expand_relu\").output\n",
        "\n",
        "    f = [16, 32, 64, 128]\n",
        "\n",
        "    # Decoder\n",
        "    x = encoder_output\n",
        "    for i in range(1, len(skip_connection_names)+1, 1):\n",
        "\n",
        "        # Conv2D layer with upsampling\n",
        "        x_skip = encoder.get_layer(skip_connection_names[-i]).output\n",
        "        x = UpSampling2D((2, 2))(x)\n",
        "        x = Concatenate()([x, x_skip])\n",
        "\n",
        "        x = Conv2D(f[-i], (3, 3), padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "\n",
        "        x = Conv2D(f[-i],(3, 3), padding=\"same\")(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(3, (1, 1), padding=\"same\")(x)\n",
        "    x = Activation(\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9eb1093c",
      "metadata": {
        "id": "9eb1093c"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "model = create_model(IMAGE_SIZE)\n",
        "opt = tf.keras.optimizers.Nadam(LR)\n",
        "metrics = [dice_loss, Recall(), Precision()]\n",
        "model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='mobilenetv2_architecture.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278197d4",
      "metadata": {
        "id": "278197d4"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-6, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "    #ModelCheckpoint(\"./best_31.h5\", save_weights_only=False, monitor='val_loss', mode='min')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc553db3",
      "metadata": {
        "id": "bc553db3"
      },
      "outputs": [],
      "source": [
        "train_steps = len(train_x)//BATCH\n",
        "valid_steps = len(valid_x)//BATCH\n",
        "\n",
        "if len(train_x) % BATCH != 0:\n",
        "    train_steps += 1\n",
        "if len(valid_x) % BATCH != 0:\n",
        "    valid_steps += 1\n",
        "\n",
        "print(f\"Train steps: {train_steps}, Valid steps: {valid_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history =  model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=valid_steps,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "As-DV-HTIT02"
      },
      "id": "As-DV-HTIT02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('SavedModels5/model_Mar_18_xyz_3cascade', save_format='tf')\n",
        "# model.save('SavedModels5/model_Sept_10_xyz', save_format='h5')"
      ],
      "metadata": {
        "id": "gtM5g5uZJdmU"
      },
      "id": "gtM5g5uZJdmU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combining trained model and new model"
      ],
      "metadata": {
        "id": "ofylaniJI0It"
      },
      "id": "ofylaniJI0It"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2a9d60",
      "metadata": {
        "id": "5c2a9d60",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "custom_layers = {\n",
        "    #\"CustomRotationLayer\": CustomRotationLayer,\n",
        "    \"dice_loss\": dice_loss\n",
        "}\n",
        "\n",
        "trained_model = tf.keras.models.load_model('SavedModels5/model_Feb_12_xyz_3cascade', custom_objects=custom_layers)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_model(trained_model, to_file=\"best_model.png\", show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "kVgTrR5CbFwC"
      },
      "id": "kVgTrR5CbFwC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Add, Concatenate\n",
        "\n",
        "def clone_layer(layer):\n",
        "    new_layer_name = f\"new_{layer.name}\"\n",
        "    layer_config = layer.get_config()\n",
        "    layer_config[\"name\"] = new_layer_name\n",
        "    new_layer = layer.__class__.from_config(layer_config)\n",
        "    new_layer.build(layer.input_shape)\n",
        "\n",
        "    original_weights = layer.get_weights()\n",
        "\n",
        "    if len(original_weights) == 2:\n",
        "        weights, biases = original_weights\n",
        "        new_layer.set_weights([weights, biases])\n",
        "    elif len(original_weights) == 1:\n",
        "        new_layer.set_weights(original_weights)\n",
        "\n",
        "    return new_layer\n",
        "\n",
        "def combine_models(model_a, model_b):\n",
        "    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"shared_input\")\n",
        "\n",
        "    # Process input through Model A (Best model)\n",
        "    outputs_a = {}\n",
        "    current_used_layers = {}\n",
        "    best_used_layers = {}\n",
        "    x_a = input_tensor\n",
        "\n",
        "    for layer in model_a.layers:\n",
        "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
        "            continue\n",
        "\n",
        "        if isinstance(layer, (tf.keras.layers.Add, tf.keras.layers.Concatenate)):\n",
        "            x_1 = x_a\n",
        "            if layer.input[1].name == \"input_image\":\n",
        "                x_2 = input_tensor\n",
        "            else:\n",
        "                real_name1 = layer.input[0].name.split(\"/\")[0]\n",
        "                real_name2 = layer.input[1].name.split(\"/\")[0]\n",
        "                x_2 = None\n",
        "                for layer_name, layer_output in best_used_layers.items():\n",
        "                    if layer_name in {real_name1, real_name2}:\n",
        "                        x_2 = layer_output\n",
        "                        break\n",
        "                if x_2 is None:\n",
        "                    raise ValueError(f\"Layer inputs {real_name1}, {real_name2} not found in best_used_layers.\")\n",
        "\n",
        "            x_a = layer([x_1, x_2])\n",
        "            best_used_layers[layer.name.split(\"/\")[0]] = x_a\n",
        "            continue\n",
        "\n",
        "        x_a = layer(input_tensor if layer.name == \"Conv1\" else x_a)\n",
        "        outputs_a[layer.name] = (layer, x_a)  # Store (layer object, output tensor) pairs\n",
        "        best_used_layers[layer.name] = x_a\n",
        "\n",
        "    x_b = input_tensor\n",
        "\n",
        "    # Process input through Model B (Current Model), inserting Model A's outputs where types match\n",
        "    for layer in model_b.layers:\n",
        "        print(f\"\\nProcessing Layer: {layer.name}\")\n",
        "\n",
        "        if isinstance(layer, tf.keras.layers.InputLayer):\n",
        "            continue\n",
        "\n",
        "        new_layer = clone_layer(layer)\n",
        "\n",
        "        if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            x_b = new_layer(x_b)\n",
        "            print(f\"Final Layer Output of {layer.name}: {x_b.shape}\\n\")\n",
        "            current_used_layers[layer.name] = x_b\n",
        "            continue\n",
        "\n",
        "        if isinstance(layer, (tf.keras.layers.Add, tf.keras.layers.Concatenate)):\n",
        "            x_1 = x_b\n",
        "            print(f\"{layer.input[0].name}\")\n",
        "            print(f\"{layer.input[1].name}\")\n",
        "\n",
        "            if layer.input[1].name == \"input_image\":\n",
        "                x_2 = input_tensor\n",
        "\n",
        "            elif layer.input[1].name.split(\"/\")[0] == \"block_1_expand_relu\":\n",
        "                x_2 = current_used_layers[\"block_1_pad\"]\n",
        "\n",
        "            elif layer.input[1].name.split(\"/\")[0] == \"block_3_expand_relu\":\n",
        "                x_2 = current_used_layers[\"block_3_pad\"]\n",
        "\n",
        "            elif layer.input[1].name.split(\"/\")[0] == \"block_6_expand_relu\":\n",
        "                x_2 = current_used_layers[\"block_6_pad\"]\n",
        "\n",
        "            else:\n",
        "                real_name1 = layer.input[0].name.split(\"/\")[0]\n",
        "                real_name2 = layer.input[1].name.split(\"/\")[0]\n",
        "                print(f\"Real name 2:{real_name2}\")\n",
        "                x_2 = None\n",
        "                for layer_name, layer_output in current_used_layers.items():\n",
        "                    if layer_name in {real_name1, real_name2}:\n",
        "                        x_2 = layer_output\n",
        "                        break\n",
        "                if x_2 is None:\n",
        "                    raise ValueError(f\"Layer inputs {real_name1}, {real_name2} not found in current_used_layers.\")\n",
        "\n",
        "            print(f\"input 1: {x_1.name}\")\n",
        "            print(f\"input 2: {x_2.name}\")\n",
        "            x_b = new_layer([x_1, x_2])\n",
        "            current_used_layers[layer.name.split(\"/\")[0]] = x_b\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            for layer_a_name, (layer_a, output_a) in outputs_a.items():\n",
        "                # Skip layers that have already been concatenated\n",
        "                if layer_a_name in current_used_layers:\n",
        "                    continue\n",
        "\n",
        "                if isinstance(layer, type(layer_a)):\n",
        "                    shape_a = output_a.shape\n",
        "                    shape_b = layer.get_input_shape_at(0)\n",
        "                    print(f\"Shape a (from Model A): {shape_a}\")\n",
        "                    print(f\"Shape b (from Model B input): {shape_b}\")\n",
        "\n",
        "                    current_used_layers[layer_a_name] = x_b\n",
        "\n",
        "                    # Check if spatial dimensions match\n",
        "                    if shape_a[1:] == shape_b[1:]:\n",
        "                        print(f\"Merging current model's {x_b.name} (shape: {shape_b}) with best model's {layer_a.name} (shape: {shape_a})\")\n",
        "                        x_b = Add(name=f\"concat_{layer.name}\")([x_b, output_a])\n",
        "                    break\n",
        "\n",
        "        x_b = new_layer(x_b)\n",
        "        print(f\"Final Layer Output of {layer.name}: {x_b.shape}\\n\")\n",
        "\n",
        "    #x = Activation(\"sigmoid\")(x_b)\n",
        "\n",
        "    combined_model = Model(inputs=input_tensor, outputs=x_b)\n",
        "    return combined_model\n",
        "\n",
        "# Create the combined model\n",
        "combined_model = combine_models(trained_model, model)"
      ],
      "metadata": {
        "id": "idHTfGw-N4Bi"
      },
      "id": "idHTfGw-N4Bi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_model.summary())\n",
        "plot_model(combined_model, to_file=\"combined_model.png\", show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "HehcatqBeqrL"
      },
      "id": "HehcatqBeqrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = tf.keras.optimizers.Nadam(LR)\n",
        "metrics = [dice_loss, Recall(), Precision()]\n",
        "combined_model.compile(loss=dice_loss, optimizer=opt, metrics=metrics)\n",
        "\n",
        "for layer in combined_model.layers:\n",
        "    print(layer.name, \"Trainable:\", layer.trainable)"
      ],
      "metadata": {
        "id": "KmrsNY2s76Mm"
      },
      "id": "KmrsNY2s76Mm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history =  combined_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=100,\n",
        "    steps_per_epoch=train_steps,\n",
        "    validation_steps=valid_steps,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "EXUqXq05zfFg"
      },
      "id": "EXUqXq05zfFg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZlaoLCwCzTw_"
      },
      "id": "ZlaoLCwCzTw_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c5c996",
      "metadata": {
        "id": "c5c5c996"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_model_history(history):\n",
        "\n",
        "    if 'loss' in history.history and 'val_loss' in history.history:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "\n",
        "        # Plot validation loss\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "\n",
        "        # Set plot title and labels\n",
        "        plt.title('Model Loss Over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        # Display the plot\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"The provided history object does not contain 'loss' and 'val_loss'.\")\n",
        "\n",
        "\n",
        "plot_model_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20f522dd",
      "metadata": {
        "id": "20f522dd"
      },
      "outputs": [],
      "source": [
        "combined_model.save('SavedModels5/model_Mar_18_xyz_3cascade', save_format='tf')\n",
        "# combined_model.save('SavedModels5/model_Sept_10_xyz', save_format='h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc70d384",
      "metadata": {
        "id": "fc70d384"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Displaying the Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a23c154",
      "metadata": {
        "id": "5a23c154"
      },
      "outputs": [],
      "source": [
        "custom_layers = {\n",
        "    #\"CustomRotationLayer\": CustomRotationLayer,\n",
        "    \"dice_loss\": dice_loss\n",
        "}\n",
        "load_trained_model = tf.keras.models.load_model('SavedModels5/model_Feb_12_xyz_3cascade', custom_objects= custom_layers)\n",
        "load_combined_model = tf.keras.models.load_model('SavedModels5/model_Mar_18_xyz_3cascade', custom_objects= custom_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "650ff181",
      "metadata": {
        "id": "650ff181"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def mask_parse(mask, channel):\n",
        "    mask = np.squeeze(mask[..., channel])\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "num_pred = 50\n",
        "\n",
        "for i, (x, y) in enumerate(zip(test_x[:num_pred], test_y[:num_pred])):\n",
        "    x = read_image(x)\n",
        "    y = read_mask(y)\n",
        "\n",
        "    # Predict using the loaded model on background-removed images (new_test_x)\n",
        "    y_pred = load_trained_model.predict(np.expand_dims(test_x[i], axis=0))[0] > 0.5\n",
        "\n",
        "    h, w, _ = x.shape\n",
        "    white_line = np.ones((h, 10, 3))\n",
        "\n",
        "    all_images = [\n",
        "        x, white_line,  # Raw input image (test_x)\n",
        "        test_x[i], white_line,  # Background-removed image (new_test_x)\n",
        "        mask_parse(y, 0), white_line,  # Ground truth xy\n",
        "        mask_parse(y_pred, 0), white_line,  # Predicted xy\n",
        "        mask_parse(y, 1), white_line,  # Ground truth yz\n",
        "        mask_parse(y_pred, 1), white_line,  # Predicted yz\n",
        "        mask_parse(y, 2), white_line,  # Ground truth xz\n",
        "        mask_parse(y_pred, 2)  # Predicted xz\n",
        "    ]\n",
        "\n",
        "    # Concatenate all the images horizontally\n",
        "    image = np.concatenate(all_images, axis=1)\n",
        "\n",
        "    # Plot the image grid\n",
        "    fig = plt.figure(figsize=(25, 12))\n",
        "    a = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "    a.text(70, -5, \"Raw Image\")\n",
        "    a.text(300, -5, \"Modified Image\")\n",
        "    a.text(560, -5, \"GT XY\")\n",
        "    a.text(775, -5, \"Predicted XY\")\n",
        "    a.text(1025, -5, \"GT XZ\")\n",
        "    a.text(1240, -5, \"Predicted XZ\")\n",
        "    a.text(1490, -5, \"GT YZ\")\n",
        "    a.text(1700, -5, \"Predicted YZ\")\n",
        "\n",
        "    imgplot = plt.imshow(image)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Predict before saving\n",
        "y_pred_before = combined_model.predict(np.expand_dims(test_x[0], axis=0))[0]\n",
        "\n",
        "# Predict after loading\n",
        "y_pred_after = load_combined_model.predict(np.expand_dims(test_x[0], axis=0))[0]\n",
        "\n",
        "# Compare outputs\n",
        "print(\"Are the predictions the same?\", np.array_equal(y_pred_before, y_pred_after))\n",
        "print(f\"Min-Max before: {y_pred_before.min()} - {y_pred_before.max()}\")\n",
        "print(f\"Min-Max after: {y_pred_after.min()} - {y_pred_after.max()}\")\n"
      ],
      "metadata": {
        "id": "KnJxtnrURzgS"
      },
      "id": "KnJxtnrURzgS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def mask_parse(mask, channel):\n",
        "    mask = np.squeeze(mask[..., channel])\n",
        "    mask = [mask, mask, mask]\n",
        "    mask = np.transpose(mask, (1, 2, 0))\n",
        "    return mask\n",
        "\n",
        "num_pred = 50\n",
        "\n",
        "for i, (x, y) in enumerate(zip(test_x[:num_pred], test_y[:num_pred])):\n",
        "    x = read_image(x)\n",
        "    y = read_mask(y)\n",
        "\n",
        "    # Predict using the loaded model on background-removed images (new_test_x)\n",
        "    y_pred = load_combined_model.predict(np.expand_dims(test_x[i], axis=0))[0] > 0.5\n",
        "\n",
        "    h, w, _ = x.shape\n",
        "    white_line = np.ones((h, 10, 3))\n",
        "\n",
        "    all_images = [\n",
        "        x, white_line,  # Raw input image (test_x)\n",
        "        test_x[i], white_line,  # Background-removed image (new_test_x)\n",
        "        mask_parse(y, 0), white_line,  # Ground truth xy\n",
        "        mask_parse(y_pred, 0), white_line,  # Predicted xy\n",
        "        mask_parse(y, 1), white_line,  # Ground truth yz\n",
        "        mask_parse(y_pred, 1), white_line,  # Predicted yz\n",
        "        mask_parse(y, 2), white_line,  # Ground truth xz\n",
        "        mask_parse(y_pred, 2)  # Predicted xz\n",
        "    ]\n",
        "\n",
        "    # Concatenate all the images horizontally\n",
        "    image = np.concatenate(all_images, axis=1)\n",
        "\n",
        "    # Plot the image grid\n",
        "    fig = plt.figure(figsize=(25, 12))\n",
        "    a = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "    a.text(70, -5, \"Raw Image\")\n",
        "    a.text(300, -5, \"Modified Image\")\n",
        "    a.text(560, -5, \"GT XY\")\n",
        "    a.text(775, -5, \"Predicted XY\")\n",
        "    a.text(1025, -5, \"GT XZ\")\n",
        "    a.text(1240, -5, \"Predicted XZ\")\n",
        "    a.text(1490, -5, \"GT YZ\")\n",
        "    a.text(1700, -5, \"Predicted YZ\")\n",
        "\n",
        "    imgplot = plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NqARq_zp-vlJ"
      },
      "id": "NqARq_zp-vlJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get a single batch from the dataset generator\n",
        "x_batch, y_batch = next(iter(train_dataset))  # Fetch one batch\n",
        "\n",
        "# Extract all layers dynamically from the model's computational graph\n",
        "all_layers = []\n",
        "layer_dict = {}\n",
        "\n",
        "for layer in load_combined_model.layers:\n",
        "    try:\n",
        "        if isinstance(layer.output, list):  # If a layer has multiple outputs\n",
        "            for i, out in enumerate(layer.output):\n",
        "                layer_dict[f\"{layer.name}_{i}\"] = out\n",
        "                all_layers.append(out)\n",
        "        else:\n",
        "            layer_dict[layer.name] = layer.output\n",
        "            all_layers.append(layer.output)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {layer.name} due to error: {e}\")\n",
        "\n",
        "# Debug: Print all detected layers\n",
        "print(\"\\nCaptured Layer Outputs:\")\n",
        "for i, (layer_name, output) in enumerate(layer_dict.items()):\n",
        "    print(f\"{i+1}: {layer_name} -> Output Shape: {output.shape}\")\n",
        "\n",
        "# Create a model for visualization, capturing all layers dynamically\n",
        "visualization_model = tf.keras.models.Model(inputs=load_combined_model.input, outputs=list(layer_dict.values()))\n",
        "\n",
        "# Run the batch through the visualization model\n",
        "layer_outputs = visualization_model.predict(x_batch)\n",
        "\n",
        "# Plot all layer outputs\n",
        "num_layers = len(layer_outputs)\n",
        "cols = 5\n",
        "rows = (num_layers // cols) + (1 if num_layers % cols != 0 else 0)\n",
        "\n",
        "plt.figure(figsize=(25, 5 * rows))\n",
        "\n",
        "for i, (layer_name, output) in enumerate(zip(layer_dict.keys(), layer_outputs)):\n",
        "    plt.subplot(rows, cols, i + 1)\n",
        "    plt.title(f\"{layer_name}\\nShape: {output.shape}\", fontsize=10)\n",
        "\n",
        "    if len(output.shape) == 4:  # Feature maps\n",
        "        feature_map = output[0, :, :, 0]  # First feature map from the batch\n",
        "        plt.imshow(feature_map, cmap='viridis')\n",
        "        plt.axis(\"off\")\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, f\"Output Shape: {output.shape}\", ha='center', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mlU_VgwumIk7"
      },
      "id": "mlU_VgwumIk7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}